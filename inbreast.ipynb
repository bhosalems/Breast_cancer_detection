{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee1693d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pandas import DataFrame, read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import cv2\n",
    "from keras.preprocessing.image import load_img,img_to_array\n",
    "from keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.applications import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.preprocessing.image import *\n",
    "from tensorflow.keras.utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be39da02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2987e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list=[]\n",
    "image_list=[]\n",
    "file = r'./inbreast/INbreast.xls'\n",
    "df = pd.read_excel(file,nrows=410)\n",
    "for path in glob('./inbreast/All-PNGs/*.png'):\n",
    "    name=int(path.split('/')[3].split('_')[0])\n",
    "    img = cv2.imread(path)\n",
    "    img=cv2.resize(img,(1200,1200))\n",
    "    class_list.append(df[df['File Name'] == name]['Bi-Rads'].iloc[0])\n",
    "    image_list.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0f3fcd9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, '4a', 2, '4c', 2, 1, 2, 2, 1, 3, 2, 1, 2, '4c', 2, 1, 2, 1, 2, 2, 2, 2, 2, 5, 2, '4c', 1, 2, 2, 1, 3, 1, 2, 3, 2, 2, 2, 2, 2, '4b', 1, 3, 2, 3, 2, 2, 2, 3, 5, 1, 2, 3, 2, '4c', 1, 2, '4c', 2, 2, 2, 2, 3, 2, 2, '4b', 2, 2, 2, 2, '4a', 2, 5, 5, 5, 2, 5, 1, 2, '4a', 1, '4c', 2, 2, 1, 2, '4c', 1, 2, 2, 2, '4b', 2, 5, 2, 2, 1, 2, 2, 1, 5, 1, 2, 2, 2, 2, 2, 2, '4a', 2, 5, 2, 1, 2, 1, 3, 2, 1, 5, 2, 3, '4a', 2, 1, 2, 2, 1, 1, 2, '4a', 5, 2, 2, 5, 2, 1, '4a', 2, '4c', 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 5, 1, 2, 5, 1, 3, 2, 2, 5, 1, 2, 2, 6, 3, 2, 2, 2, 1, 1, 2, 2, 2, 2, 6, 2, 1, '4c', 1, 2, 2, 2, 1, 5, 6, 2, 2, 2, 5, 2, 2, 1, 5, 2, 2, 2, 1, 5, 5, '4b', 5, '4c', 2, 1, 5, 5, '4a', 3, 2, 3, 2, '4c', 2, 2, 5, 2, 2, 1, 2, 1, 1, 2, 5, 2, 2, 5, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 5, '4c', 1, 2, 5, 3, 2, 2, 2, 2, 2, 5, 2, 2, 1, 1, 1, 5, 2, 3, 2, 1, 2, 2, 1, 1, 2, 2, 2, '4c', 2, 2, 2, 2, 5, 2, 2, 2, 2, 1, 1, 6, 6, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, '4c', 2, 2, 2, 1, '4b', 2, 6, 6, 2, 5, '4c', 2, 1, '4c', 1, 2, 2, 5, '4a', '4b', 1, 1, 2, 2, '4a', '4c', 3, 2, '4a', 2, 2, 2, 2, 2, 2, 3, 5, '4c', 2, 2, '4b', 5, '4a', 2, 2, 2, 2, 1, 2, 2, 5, 2, 2, 5, 3, 2, 1, 5, 1, 5, '4a', 1, 5, 2, 2, 2, 2, 2, 2, '4b', '4c', 2, 3, '4c', 5, 2, 2, 1, 2, 6, 2, 2, 2, 2, 5, 3, 5, 2, 2, 2, '4c', 1, 5, 5, 5, 2, 2, 2, 3, '4c', 5, 2, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e7a389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 5, 6, '4b', '4a', '4c'}\n",
      "{1: 0, 2: 1, 3: 2, 5: 3, 6: 4, '4b': 5, '4a': 6, '4c': 7}\n",
      "(410, 1200, 1200, 3)\n",
      "{1: 0, 2: 1, 3: 2, 5: 3, 6: 4, '4b': 5, '4a': 6, '4c': 7}\n"
     ]
    }
   ],
   "source": [
    "labels = set(class_list)\n",
    "print(labels)\n",
    "label2class = {}\n",
    "new_class_list =[]\n",
    "c = 0\n",
    "for l in labels:\n",
    "    label2class[l] = c\n",
    "    c+=1\n",
    "print(label2class)\n",
    "for l in class_list:\n",
    "    new_class_list.append(label2class[l])\n",
    "X=np.array(image_list)\n",
    "Y=np.array(new_class_list)\n",
    "print(X.shape)\n",
    "print(label2class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cd55722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6, 1, 7, 1, 0, 1, 1, 0, 2, 1, 0, 1, 7, 1, 0, 1, 0, 1, 1, 1, 1, 1, 3, 1, 7, 0, 1, 1, 0, 2, 0, 1, 2, 1, 1, 1, 1, 1, 5, 0, 2, 1, 2, 1, 1, 1, 2, 3, 0, 1, 2, 1, 7, 0, 1, 7, 1, 1, 1, 1, 2, 1, 1, 5, 1, 1, 1, 1, 6, 1, 3, 3, 3, 1, 3, 0, 1, 6, 0, 7, 1, 1, 0, 1, 7, 0, 1, 1, 1, 5, 1, 3, 1, 1, 0, 1, 1, 0, 3, 0, 1, 1, 1, 1, 1, 1, 6, 1, 3, 1, 0, 1, 0, 2, 1, 0, 3, 1, 2, 6, 1, 0, 1, 1, 0, 0, 1, 6, 3, 1, 1, 3, 1, 0, 6, 1, 7, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 3, 0, 1, 3, 0, 2, 1, 1, 3, 0, 1, 1, 4, 2, 1, 1, 1, 0, 0, 1, 1, 1, 1, 4, 1, 0, 7, 0, 1, 1, 1, 0, 3, 4, 1, 1, 1, 3, 1, 1, 0, 3, 1, 1, 1, 0, 3, 3, 5, 3, 7, 1, 0, 3, 3, 6, 2, 1, 2, 1, 7, 1, 1, 3, 1, 1, 0, 1, 0, 0, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 3, 7, 0, 1, 3, 2, 1, 1, 1, 1, 1, 3, 1, 1, 0, 0, 0, 3, 1, 2, 1, 0, 1, 1, 0, 0, 1, 1, 1, 7, 1, 1, 1, 1, 3, 1, 1, 1, 1, 0, 0, 4, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 7, 1, 1, 1, 0, 5, 1, 4, 4, 1, 3, 7, 1, 0, 7, 0, 1, 1, 3, 6, 5, 0, 0, 1, 1, 6, 7, 2, 1, 6, 1, 1, 1, 1, 1, 1, 2, 3, 7, 1, 1, 5, 3, 6, 1, 1, 1, 1, 0, 1, 1, 3, 1, 1, 3, 2, 1, 0, 3, 0, 3, 6, 0, 3, 1, 1, 1, 1, 1, 1, 5, 7, 1, 2, 7, 3, 1, 1, 0, 1, 4, 1, 1, 1, 1, 3, 2, 3, 1, 1, 1, 7, 0, 3, 3, 3, 1, 1, 1, 2, 7, 3, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(new_class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "699f9b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, True, False, True, True, True, False, True, False, False, True, False, True, False, False, False, False, True, False, False, False, False, True, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, True, True, False, False, True, False, False, True, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, True, True, False, False, False, True, False, False, False, True, False, False, False, False, True, True, True, True, True, False, False, True, True, True, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, True, True, False, True, True, False, False, True, False, False, False, True, True, True, False, False, False, False, True, True, False, False, True, False, False, False, False, False, False, False, True, True, False, False, True, True, True, False, False, False, False, False, False, False, True, False, False, True, False, False, False, True, False, True, True, False, True, False, False, False, False, False, False, True, True, False, False, True, True, False, False, False, False, True, False, False, False, False, True, False, True, False, False, False, True, False, True, True, True, False, False, False, False, True, True, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "new_class_list_1=[False if i<3 else True for i in new_class_list]\n",
    "print(new_class_list_1)\n",
    "Y=np.array(new_class_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "558d4c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(287, 1200, 1200, 3)\n",
      "(41, 1200, 1200, 3)\n",
      "(82, 1200, 1200, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test1, y_train, y_test1 = train_test_split(X, Y, test_size=0.3, random_state=42,shuffle=True)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test1, y_test1, test_size=0.33, random_state=42,shuffle=True)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_val.shape)\n",
    "x_train_normalize = x_train.astype('float32') / 255.0\n",
    "x_test_normalize = x_test.astype('float32') / 255.0\n",
    "x_val_normalize = x_val.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca6bdc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(zca_whitening=False,rotation_range=40,width_shift_range=0.2,height_shift_range=0.2\n",
    "                             ,shear_range=0.2,zoom_range=0.2,horizontal_flip=True,fill_mode='nearest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c544dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG19(input_shape=(1200,1200,3), weights='imagenet', include_top=False)\n",
    "model=Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256,kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256,kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,validation_split=0.2,shuffle=True, epochs=10,validation_data=(x_val_normalize, y_val))\n",
    "loss_value , accuracy = model.evaluate(x_test, y_test)\n",
    "print('Test_loss_value = ' +str(loss_value))\n",
    "print('test_accuracy = ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e738549",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 163s 16s/step - loss: 447.9290 - accuracy: 0.6934 - val_loss: 335.3661 - val_accuracy: 0.8049\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 161s 17s/step - loss: 251.4668 - accuracy: 0.7561 - val_loss: 168.6308 - val_accuracy: 0.8049\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 163s 17s/step - loss: 169.3449 - accuracy: 0.7422 - val_loss: 159.2979 - val_accuracy: 0.8049\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 161s 17s/step - loss: 143.6761 - accuracy: 0.7561 - val_loss: 124.1903 - val_accuracy: 0.8049\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 159s 17s/step - loss: 127.3820 - accuracy: 0.7596 - val_loss: 123.9366 - val_accuracy: 0.8049\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 162s 17s/step - loss: 123.0112 - accuracy: 0.7561 - val_loss: 113.1848 - val_accuracy: 0.8049\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 161s 17s/step - loss: 117.9274 - accuracy: 0.7526 - val_loss: 119.1568 - val_accuracy: 0.8049\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 157s 17s/step - loss: 119.4654 - accuracy: 0.7561 - val_loss: 111.3762 - val_accuracy: 0.8049\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 160s 17s/step - loss: 116.1621 - accuracy: 0.7282 - val_loss: 117.7311 - val_accuracy: 0.8049\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 160s 17s/step - loss: 116.8945 - accuracy: 0.7561 - val_loss: 111.0386 - val_accuracy: 0.8049\n",
      "2/2 [==============================] - 3s 661ms/step - loss: 162.0325 - accuracy: 0.6585\n",
      "162.032470703125 0.6585366129875183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fac831e5ac0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 10\n",
    "class_weight = {True: 1.,\n",
    "                False: 3}\n",
    "filename='twolayer.csv'\n",
    "history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
    "# batch_size = 32\n",
    "cnn_model=tf.keras.Sequential([\n",
    "tf.keras.layers.Conv2D(16,(5,5),input_shape=(1200,1200,3),activation='relu'),\n",
    "tf.keras.layers.MaxPooling2D(2,2), # First Convolution and Pooling Layers\n",
    "tf.keras.layers.Conv2D(32,(5,5),activation='relu'),\n",
    "tf.keras.layers.MaxPooling2D(2,2), # Second Convolution and Pooling Layers\n",
    "tf.keras.layers.Dropout(0.3), # First dropout\n",
    "# tf.keras.layers.Conv2D(32,(2,2),activation='relu'),\n",
    "# tf.keras.layers.MaxPooling2D(2,2), # Third Convolution and Pooling Layers\n",
    "# tf.keras.layers.Dropout(0.3), # Second dropout\n",
    "])\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(32, activation='relu', kernel_initializer='he_uniform',kernel_regularizer=regularizers.L1(0.01)))\n",
    "cnn_model.add(Dense(1, activation='sigmoid',kernel_regularizer=regularizers.L1(0.01)))\n",
    "opt = Adam(learning_rate=0.001)\n",
    "cnn_model.compile(optimizer = opt , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n",
    "# history = cnn_model.fit(x_train, y_train,validation_data=(x_val,y_val),epochs=10)\n",
    "history = cnn_model.fit(datagen.flow(x_train_normalize, y_train),epochs=epochs,\n",
    "                        validation_data=(x_val_normalize, y_val),class_weight=class_weight,callbacks=[history_logger],)\n",
    "loss_value , accuracy = cnn_model.evaluate(x_test,y_test)\n",
    "print(loss_value,accuracy)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=y_train\n",
    "val_y=y_val\n",
    "test_y=y_test\n",
    "ann_model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = ann_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=100)\n",
    "loss_value , accuracy = ann_model.evaluate(train_features, train_y)\n",
    "\n",
    "# print('Train_accuracy is:' + str(accuracy))\n",
    "# loss_value , accuracy = ann_model.evaluate(val_features, val_y)\n",
    "# print('Validation_accuracy is := ' + str(accuracy))\n",
    "# loss_value , accuracy = ann_model.evaluate(test_features, test_y)\n",
    "# print('test_accuracy is : = ' + str(accuracy))\n",
    "# print(\"Performance Report:\")\n",
    "# y_pred1=ann_model.predict(test_features)\n",
    "\n",
    "# target=[\"0\",\"1\"]\n",
    "# from sklearn import metrics\n",
    "# print('Accuracy score is :', np.round(metrics.accuracy_score(test_y, y_pred1),4))\n",
    "# print('Precision score is :', np.round(metrics.precision_score(test_y, y_pred1, average='weighted'),4))\n",
    "# print('Recall score is :', np.round(metrics.recall_score(test_y,y_pred1, average='weighted'),4))\n",
    "# print('F1 Score is :', np.round(metrics.f1_score(test_y, y_pred1, average='weighted'),4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c9c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "image_size = 800  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "num_classes = 100\n",
    "input_shape = (800, 800, 3)\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802cd5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size,image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d75d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73441c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a981520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84aca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "vit_classifier = create_vit_classifier()\n",
    "history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba03b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
